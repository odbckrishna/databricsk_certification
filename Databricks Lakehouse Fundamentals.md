## One of the key features delivered by the Databricks Lakehouse platform is ACID transactions. What describes ACID transactions?

> - They ensure that data never falls into an inconsistent state because of an operation that only partially completes
> - They identify rare events or observations in data which are statistically different from the rest of the observations
> - They supervise data transactions to ensure that data is not being misused
> - **correct )    They are data structures that organize data into a 2-dimensional table of rows and columns, much like a spreadsheet**

## What is true about an organization’s data when they use Databricks?

> - Data that is stored while it’s being processed by Databricks is always encrypted
> - **correct )     Data is always stored in their cloud account**
> - Data is always stored in the Databricks cloud account
> - Organizations must encrypt data to maximize security

## Where does Databricks Machine Learning fit into the Databricks Lakehouse Platform?

> - **correct )     It is one of the core services of the Lakehouse Platform, tailored towards data practitioners building and managing machine learning models**
> - It is one of the core services of the Lakehouse Platform, tailored towards data practitioners who need to query data and publish visual insights
> - It is one of the core services of the Lakehouse Platform, tailored towards data practitioners building data pipelines to make data available to everyone in an organization
> - It is one of the core services of the Lakehouse Platform, tailored towards data practitioners who need to manage users and workspace governance

## Which statement holistically describes a Lakehouse?

> - It is the latest data management paradigm that combines elements from data warehouses and data lakes
> - It is an established data management tool that houses primarily unstructured data
> - **correct )    It is a unified analytics platform used for data management, processing, and analytics**
> - It is a relatively new data management system that stores all data in its raw state

## Which scenario would be best tackled using Data Science and Engineering Workspace?

> - Setting up access controls to limit data visibility to a particular group within an organization
> - Tracking and comparing the results of machine learning experiments
> - Creating a dashboard that will alert business managers of important changes in daily sales revenue
> - **correct )    Using Databricks Notebooks to collaborate with team members in a variety of programming languages**

## Which scenario would be best tackled using Databricks Machine Learning?

> - Setting up access controls to limit data visibility to a particular group within an organization
> - **correct )    Tracking and comparing the results of machine learning experiments.**
> - Creating a dashboard that will alert business managers of important changes in daily sales revenue
> - Replacing data silos with a single home for structured, semi-structured, and unstructured data

## The Databricks Lakehouse Platform is built on top of some of the world’s most successful open-source data projects. Which of the following open source projects were originally created by Databricks and come as managed versions in the Databricks Lakehouse Platform?

> - **correct )    Delta Lake, MLflow, Koalas**
> - Delta Lake, Docker, Cloudera
> - Docker, Redash, Cloudera
> - MLflow, Redash, GitLab

## What does Databricks Data Science and Engineering Workspace allow data practitioners to do?

> - Build a centralized repository and registry for features, or input variables, for machine learning models
> - Use a specialized query interface to view all queries and dashboards created within an organization
> - Use MLflow to explore and develop features to include in machine learning model development
> - **correct )    Integrate Databricks notebooks into a CI/CD workflow with Databricks Repos and Databricks Workflows**

## Delta Lake ensures data governance through Unity Catalog. What does this refer to?

> - Enforces data constraints to satisfy business requirements
> - **correct )    Enforces access control lists on clusters, notebooks, tables and views**
> - Enforces access control lists on tables and views
> - Enforces access control lists on clusters and notebooks

## What defines Delta Lake?

> - It is an open protocol for secure real-time exchange of large datasets, which enables secure data sharing across products for the first time
> - **correct )    It is the storage format that data is put into that integrates with all data processing engines used within an organization**
> - It helps data engineering teams simplify ETL development and management with declarative pipeline development, automatic data testing, and deep visibility for monitoring and recovery
> - It is a fine-grained, centralized security model for data lakes across clouds that enables users to share, audit, and manage structured and unstructured data

## What is the access point to the Databricks Lakehouse Platform for machine learning practitioners?

> - Databricks Data Science and Engineering Workspace
> - Databricks Delta Lake
> - Databricks SQL
> - **correct )    Databricks Machine Learning**

## What does Databricks SQL allow data practitioners to do?

> - Build a centralized repository and registry for features, or input variables, for machine learning models
> - Explore and develop features to include in machine learning model development
> - View all queries and dashboards created within an organization
> - **correct )    Use SQL commands to perform ad-hoc and exploratory data analysis on an organization’s data lake**

## One of the key features delivered by the Databricks Lakehouse platform is that data storage is decoupled from compute. What is a direct benefit of decoupling storage from compute?

> - Storage costs are decreased since less overhead is expended storing information related to the compute environment
> - **correct )    The cost of storage and compute are managed separately and can be scaled to accommodate more concurrent users or larger datasets independently**
> - Compute costs are decreased since less overhead is expended managing storage
> - Accessing storage is more performant since it can be distributed more freely

## Which scenario would be best tackled using Databricks SQL?

> - Setting up access controls to limit data visibility to a particular group within an organization
> - Replacing data silos with a single home for structured, semi-structured, and unstructured data
> - **correct )    Creating a dashboard that will alert business managers of important changes in daily sales revenue**
> - Tracking the results of machine learning experiments

## What describes how the Databricks Lakehouse Platform functions within an organization, at a high level?

> - Real-time data is captured in Delta Lake and ultimately lands in an organization’s data warehouse. That data is then used by data practitioners for their data use cases.
> - Streaming data lands in multiple data warehouses. It is filtered through Delta Lake, which determines which data warehouse the data should be kept in. That data is then used by data practitioners for their data use cases.
> - **correct )    A variety of data types land in an organization’s open data lake. Delta Lake augments that data lake by providing structure and governance to that data. That data is then used by data practitioners for their data use cases.**
> - Structured, semi-structured, and unstructured data are captured in Delta Lake. The data is then passed to a data lake, where it is given structure and munged for quality. That data is then used by data practitioners for their data use cases.

## One of the key features delivered by the Databricks Lakehouse platform is data schema enforcement. What describes data schema enforcement?

> - It ensures data quality by rejecting data ingestion into an organization if a dataset’s metadata doesn’t contain adequate details, as defined by an organization
> - It guarantees data freshness by limiting the data that lands in an organization’s open data lake, based on whether or not it is GDPR compliant
> - It guarantees data freshness by eliminating duplicative data found within a database
> - **correct )    It ensures data quality by rejecting writes to a data table that do not match the way that data is structured and organized in that table**

## What does the Databricks Lakehouse Platform provide to data teams?

> - An environment called a Lakehouse, which combines elements from data lakes and EDSS systems
> - A toolset of compatible, open-source technologies to streamline collaboration among team members, depending on their role
> - **correct )    One centralized user interface so that data practitioners work in the same environment**
> - A completely open-source environment that guarantees seamless integration with the existing tools being used by an organization

## Delta Lake ensures data quality by enabling an organization to do what?

> - Structuring queries to act as if they are performed in a single operation
> - Applying constraints on the data to ensure that expectations will be met
> - **correct )    Providing access to data to only those who should have access**
> - Ordering table data

## What are the primary services that comprise the Databricks Lakehouse Platform?

> - Databricks SQL, Apache Spark, Delta Lake
> - Unity Catalog, Databricks Notebooks, Databricks Repos
> - Delta Lake, Apache Spark, Databricks Security & Governance
> - **correct )    Databricks SQL, Databricks Machine Learning, Databricks Data Science and Data Engineering Workspace**

## Which is true about Databricks?

> - It provides organizations with out-of-the-box solutions for managing IT infrastructure and is primarily aimed at IT administrators
> - It provides organizations with robust data warehouses and is primarily aimed at data engineers and data architects
> - **correct )    It provides organizations with a collaborative platform to work with big data, and is primarily aimed at data engineers, data scientists, data analysts, and machine learning practitioners**
> - It provides organizations with open data lakes and robust data warehouses, primarily aimed at administrators of big data infrastructure
